
\chapter{PIC算法在GPU上的实现}
\label{chap:PIC_GPU}

我们已经在\ref{section:PIC_algorithm}中介绍了基本的PIC算法。在本章中，我们致力于在GPU上实现基于PIC算法的多粒子跟踪程序的并行化，提高程序的运行效率。

在本章中，首先，我们会介绍GPU程序的基本结构和并行策略。之后，我们做了一个GPU程序和CPU程序结果的对比，以验证程序的正确性。再后，我们展示了GPU程序的效率和在不同规模的问题下的加速比。最后，我们也实现了此程序在新的CPU架构，Knight Landing，上的并行化运行，并与GPU的运行效率进行了对比。

\section{GPU程序结构}
在\ref{section:PIC_algorithm}中我们介绍了，PIC算法主要包括四部分：权重差值，解泊松方程，将场反权重到粒子，推动粒子。
在GPU上实现PIC算法，主要难点在第一步权重差值上。第一步权重插值是将粒子逐个权重到网格上的过程，此步骤多线程并行运行时，会遇到不同线程相互竞争的问题（race condition），从而导致错误的结果。

线程竞争是指在程序中，协作的线程可能共享一些彼此都能读写的公用存储区，而竞争出现在当两个或多个线程同时访问同一内存地址，并且尝试写入数据的时候。由于线程调度器会以任意顺序运行多个线程，而我们无法得知线程尝试访问共享内存的顺序。因此，数据的最红结果会取决于线程调度器的算法，即多个线程“竞争”访问和改变数据。

图\ref{fig:PIC_weighting}显示了PIC算法中权重差值部分的运算流程和多线程下可能会出现的竞争和冲突结果，其中蓝色点代表粒子，而绿色星状点代表与粒子相近的网格。使用多个线程运行权重插值时，通常使不同的线程处理不同的粒子。假设我们使用五个时钟锁步线程，如图\ref{fig:PIC_weighting}中所示，线程A-E分别处理五个粒子。A，B，C三个线程并没有发生冲突，但是D线程和E线程会产生竞争，造成错误的运算结果。在第一个时钟周期内，线程D和E分别从同一内存地址M，读取格点原本的电荷量V；在第二个时钟周期，线程D读取粒子D的电荷并计算相应应该加到格点的电荷V+D，而线程E读取粒子E并计算得到V+E；在第三个时钟周期，线程D将V+D写入内存地址M，同时线程E将V+E写入同一内存地址M。但是两个写入都是错误，正确的结果应该是V+D+E。

\begin{figure}[!htb]
  \centering
  \begin{tabular}{|l|l|}
    \multicolumn{2}{c}{
    \includegraphics[width=0.65\textwidth]{Img/3PIC_weighting.pdf}} \\
  \end{tabular}
  \begin{tabular}{|l|l|}
    \hline
    Thread D & Thread E  \\
    \hline
    1D: Read variable V     & 1E: Read variable V     \\
    2D: Add D to variable V & 2E: Add E to variable V \\
    3D: Write back (V+D)    & 3E: Write back (V+E)    \\
    \hline
    \multicolumn{2}{c}{But it should be V + D + E!}
  \end{tabular}
  \caption{权重差值中的线程冲突}
  \label{fig:PIC_weighting}
\end{figure}

为了避免线程之间的竞争和冲突，我们采取先将网格分块，再随后将网格上的电荷合并的做法。如图\ref{fig:PIC_tile}所示，绿色部分为一个区块，而一个线程单独处理一个区块。想要分块处理，粒子必须是有序的，这样才能使相应的线程找到所对应区域的粒子。所以在插值权重之前，我们需要对粒子进行排序（reorder）。假设区块数目为N，我们需要声明N个数组，以分别对应N个区块中的粒子。在程序初始阶段，我们将粒子数据拷贝到不同的区块数组中，并且在随后的每一步中，粒子可能改变位置，所以我们要对粒子重新排序，以确保粒子处于所对应的区块数组。用这种方法，每一个线程能够单独处理自己所分配的区块，从而避免了线程之间的竞争和冲突。加入了排序之后的PIC算法流程如图\ref{fig:PIC_flow_reorder}所示，其中，标识为红色部分为粒子排序，是PIC算法在GPU上并行运行所必须的前置步骤；而黄色部分的GPU算法和普通CPU算法有较大区别，我们在下面，对流程图种的黄色部分，即GPU上的PIC算法中的粒子排序，权重插值，解泊松方程，以及粒子推动做进一步讨论。

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.85\textwidth]{Img/3PIC_tile.pdf}
    \caption{网格分块}
    \label{fig:PIC_tile}
\end{figure}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.75\textwidth]{Img/3PIC_flow_reorder.pdf}
    \caption{GPU上的PIC算法分段流程}
    \label{fig:PIC_flow_reorder}
\end{figure}



\subsection{粒子排序}
\label{section:PIC_GPU_reorder}
如上所述，GPU上PIC算法中，我们必须要将粒子在每一步进行重新排序，以避免线程竞争。排序在GPU中的实现并不直接，因为其高度不规则的并且难以并行执行。在这里，我们使用临时缓冲区作为中继，以实现粒子排序算法在GPU上的并行运行。
我们将整个网格划分为不同的区块，使每个线程只需处理对应的区块即可。例如，在一个网格数为64*64*64的模拟中，我们将其分割成大小为4*4*4的区块，总共16*16*16=4096个区块，这样我们可以避免线程的竞争。

在程序初始化的时候，我们首先需要为每一个区块声明数组并为其分配内存空间，如以下代码所示。
其中，我们根据实际使用的GPU的可用内存大小来确定每个区块的最大粒子数目。如果某一区块的粒子数目超过了最大粒子数，程序会报错并停止。在这种情况下，使用更少的粒子数目或者使用内存更大的显卡可以解决问题。
因为在实际模拟中，每个区块的粒子数一般并不相同，使用内存预分配会导致浪费很多内存空间。但是不同于CPU内存，在GPU上分配内存耗时很长，所以无法在每一步计算中根据实际情况来进行内存的分配，只能采用预分配的方法，以提高程序运行效率。
\begin{lstlisting}
  const int lth =BLOCKSIZE;
  const int dim =_dimension;
  int mth       = (mx1*my1*mz1-1)/lth+1;    //number of tiles

  double *ptc;       //tiled particle
  int    *kpic;      //number of particles in each tile
  int    *nhole;     //number of hole left
  int    *ncl;       //ndirec[k*26 + i]: number of particle going
                     //to destination i[0-26) from tile k
  double *pbuff;     //buffer for transfer particles

  //get the maximum number of particle according to the GPU memory size
  size_t freeMem,totalMem;
  cuda_SafeCall(cudaMemGetInfo(&freeMem,&totalMem));
  int npm = freeMem / 2 / sizeof(double) / dim / (mx1*my1*mz1);
  if(npm>numberOfParticle) npm = numberOfParticle;
  int MaxPtcTransf = npm / 2;


  //allocate memory for particle array
  size_t ptc_mem_size = sizeof(double)*lth*dim*npm*mth;
  cuda_SafeCall(cudaMalloc((void**) &ptc,     ptc_mem_size));

  //allocate memory for particle counting array
  size_t kpic_mem_size=sizeof(int)*mx1*my1*mz1;
  cuda_SafeCall(cudaMalloc((void**) &kpic,    kpic_mem_size));

  //allocate memory for holes at tiles array
  size_t nhole_mem_size = sizeof(double)*lth*2*(MaxPtcTransf+1)*mth;
  cuda_SafeCall((cudaMalloc((void**) &nhole,  nhole_mem_size));

  //allocate memory for particle buffer array
  size_t pbuff_mem_size = sizeof(double)*lth*dim*MaxPtcTransf*mth;
  cuda_SafeCall((cudaMalloc((void**) &pbuff,  pbuff_mem_size));

  //allocate memory for transferred particle counting array
  size_t ndirec_mem_size = sizeof(int)*mx1*my1*mz1*(3*3*3-1);
  cuda_SafeCall((cudaMalloc((void**) &ndirec, ndirec_mem_size));
\end{lstlisting}

在之后的每一步计算中，在空间电荷效应之前，我们都需要根据粒子空间位置对粒子进行排序，排序流程如图\ref{fig:PIC_reorder}所示：
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.75\textwidth]{Img/3PIC_reorder.pdf}
    \caption{粒子排序示意图}
    \label{fig:PIC_reorder}
\end{figure}

首先，每个线程统计对应区块中将要离开当前区块的粒子信息，并使用数组\verb"nhole"和\verb"ndirec"记录粒子的序号和每个方向的数目。如图\ref{fig:PIC_reorder}中黄色箭头所示。如上面代码所示，我们使用最大粒子数目的一半作为最大穿越数目，而\verb"nhole"已经根据最大穿越数目进行了内存预分配，同样的，如果离开当前区块的粒子数目超过了最大穿越数目，程序会报错停止，这时候需要使用更少的粒子数目或者更大的GPU内存。

之后，我们将离开当前区块的粒子信息拷贝到一个缓冲区\verb"pbuff"。作为全局内存，\verb"pbuff"同样根据最大穿越数目进行了内存预分配。\verb"ndirec"包括了每个方向上的粒子数目，对\verb"ndirec"进行迭代求和（running sum），我们可以得到每个方向上的内存初始位置，以便于将前往某个方向的粒子在\verb"pbuff"中连续排布。

最后，对于每个区块，经过第一步的统计，我们可以知道会有多少粒子进入，以及它们在\verb"pbuff"中的位置。在将粒子信息从\verb"pbuff"拷贝到当前区块\verb"ptc"的过程中，我们先填满因为粒子离开造成的空洞。如果如果进入的粒子数目大于空洞数目，即在所有空洞都被填满后，仍有粒子进入，我们将其写入在当前区块\verb"ptc"的最后；但如果进入的粒子数目小于空洞数目，即写入所有进入粒子后仍有空洞存在，则从本区块粒子数组\verb"ptc"尾部开始，依次移动粒子数据到空洞处。通过这种方式，粒子信息在内存中的连续性得到了保证。

我们对上述步骤做了一个简要总结，通过这些排序流程，使每个线程仅处理对应的区块，来确保不会出现线程竞争。

\begin{itemize}
  \item 首先，对本区块内的粒子进行遍历，将因为位置改变而不再属于本区块的粒子序号，以及其离开的方向记录下来，写入\verb"nhole" 和\verb"ndirec"。
  \item 其次，根据其序号和方向，将离开的粒子拷贝进入全局缓存区\verb"pbuff"。
  \item 最后，再根据相邻区块离开粒子的序号和方向，确定将要进入本区块的粒子，将缓存区内的数据拷贝到本区块。
\end{itemize}


\subsection{权重插值}
\label{section:PIC_GPU_depositor}
通过以上粒子重排，我们使得每一个区块的粒子相对独立，这样，我们可以使每个线程处理一个区块，从而避免了线程冲突的情况。
一个直接的权重插值流程如图所示\ref{fig:PIC_combine}：
\begin{itemize}
  \item 首先，每个线程将自己所处理的粒子权重到一个小网格上，得到本地的网格密度分布。
  \item 然后，我们将所有本地的密度分布结合到一起，得到全局密度分布。
\end{itemize}
\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.9\textwidth]{Img/3PIC_combine.pdf}
    \caption{权重插值示意图}
    \label{fig:PIC_combine}
\end{figure}

上述为单GPU的权重差值方法，在使用多个GPU的时候，我们有两种方法：一种是使不同的GPU处理不同的空间域，在最后进行通讯，这种方法在CPU上的PIC程序中很常用，被称为“域分解”；另外一种是简单直接的使所有GPU做同样的工作，我们称这种模式为“复制模式”。
下面，我们使用一个例子对这两种方法进行比较，假设使用64*64*64个格点，区块大小为4*4*4，区块的数目为16*16*16，在TITAN上使用4个GPU运行。

\subsubsection{域分解模式}
在域分解模式中，每个GPU只需要处理相对应的域。假如区块数目为16*16*16，在4个GPU上运行的话，每个GPU需要处理的域的大小为4*16*16。但是，这种模式需要预先将粒子排序，使每个GPU只处理对应空间域内的粒子，因此需要额外的通讯和计算，具体步骤如下：
\begin{enumerate}
  \item GPU之间的粒子排序
  \begin{enumerate}
    \item 挑选粒子，每个线程处理一个区块，因此我们共有4*16*16=1024个线程。然而，在TITAN上每个GPU有2688个核，线程数小于核数，我们无法充分使用GPU。
    \item GPU之间信息交换
    \begin{enumerate}
      \item 从GPU内存拷贝到CPU内存，共需要拷贝4*16*16*(nGPU-1) * nPtcMax个粒子，其中nPtcMax个粒子为最大传输粒子数。
      \item CPU和CPU之间的信息交换，我们使用MPI\_send/receive，共需要交换4*16*16* (nGPU-1) *nPtcMax个粒子。
      \item 从CPU内存拷贝到GPU内存，共需要拷贝4*16*16*(nGPU-1) * nPtcMax个粒子。
    \end{enumerate}
  \end{enumerate}
  \item GPU内部的粒子排序，每个GPU的区块数目为4*16*16=1024，小于在每个GPU上的核数2688。
  \item GPU内部的权重差值，由于第一步进行了GPU之间的粒子排序，所以每个GPU上的粒子数目不同，格点数目为16*64*64。
  \item GPU之间的格点信息归约。
  \begin{enumerate}
    \item 从GPU内存拷贝到CPU内存，共需要拷贝16*64*64个格点。
    \item CPU和CPU之间的信息交换，我们使用MPI\_Allgether，共需要交换64*64*64个格点。
    \item 从CPU内存拷贝到GPU内存，共需要拷贝64*64*64个格点。
  \end{enumerate}
\end{enumerate}

\subsubsection{复制模式}
在复制模式中，每个GPU进行同样的工作，同域分解模式相比，复制模式无法使用多GPU来节省计算时间，但是也无需在GPU之间进行粒子信息的交换。其流程如下所示，其中每一项和上面域分解模式相对。

\begin{enumerate}
  \item 无需GPU之间排序
  \item GPU内部的粒子排序，每个GPU的区块数目为16*16*16=4096，大于在每个GPU上的核数2688。
  \item GPU内部的权重差值，由于第一步进行了GPU之间的粒子排序，所以每个GPU上的粒子数目相同同，格点数目为64*64*64。
  \item GPU之间的格点信息归约。
  \begin{enumerate}
    \item 从GPU内存拷贝到CPU内存，共需要拷贝64*64*64个格点。
    \item CPU和CPU之间的信息交换，我们使用MPI\_Allreduce，共需要交换64*64*64个格点。
    \item 从CPU内存拷贝到GPU内存，共需要拷贝64*64*64个格点。
  \end{enumerate}
\end{enumerate}

可以看出，两个模式相比较的话，域分解模式在第一步中有了很多额外的通讯，在第二步与第三步中能够使用更少的格点数。然后，使用通讯时间来换取更少的计算量并不能带来效率的提升，而且，在域分解模式下，我们并不能充分利用GPU。因此，在我们的代码中以及接下来的性能测试中，我们均采用“复制模式”。

\subsection{泊松方程}
\label{section:PIC_GPU_Poisson}
在将粒子权重到网格上之后，下一步即解网格上的泊松方程。在单GPU上运行的版本我们在此不再赘述，而在多GPU上的并行版本和权重差值相似，在也存在两种方法：一种是参考经典的CPU上的PIC程序，使用不同的GPU处理不同的空间域，即“域分解模式”；另外一种是简单直接的使所有GPU做同样的工作，即“复制模式”。由于解泊松方程的运算量较大，是PIC方法中很重要的一步，我们对两种模式都进行了实现并进行了比较。

\subsubsection{域分解模式与复制模式}
域分解的优点在于，通过使用不同的GPU处理不同的空间域，每个GPU可以降低计算量，从而提高程序运行速度；缺点也很明显，域分解模式需要不同GPU之间的信息交换，而在目前GPU与GPU之间，特别是在不同节点之间，并不能直接进行信息交换，而是必须通过CPU来进行，即信息交换需要三步：
\begin{enumerate}
  \item 从GPU内存拷贝到CPU内存。
  \item CPU和CPU之间的通过MPI进行信息交换。
  \item 从CPU内存拷贝到GPU内存。
\end{enumerate}

如小节\ref{section:PIC_FFT}所示，求解泊松方程中最主要的部分是使用傅里叶变换（FFT）。在GPU代码中，我们使用NVIDIA公司的CUDA快速傅里叶变换库（cuFFT）\cite{nvidia2010cufft}在进行变换。在求解多GPU上的3维傅里叶变换时，最好是在三个方向分别做1维的傅里叶变换，并在每次变换之间对数据进行转置。

假设在X，Y，Z三个方向上的格点数分别为$N_x$，$N_y$，$N_z$，那么当执行X方向的傅里叶变换的时候，变换的数组长度即为$N_x$，变换的次数为$N_y \times N_z$。以使用四个GPU为例，那么1号GPU需要处理的数据为\verb'rho'[$N_x$][$0 \rightarrow \frac{N_y}{4}$][$N_z$]，2号GPU需要处理的数据为\verb'rho'[$N_x$][$\frac{N_y}{4} \rightarrow 2\frac{N_y}{4}$][$N_z$]，3号GPU需要处理的数据为\verb'rho'[$N_x$][$2\frac{N_y}{4}\rightarrow 3\frac{N_y}{4}$][$N_z$]，4号GPU需要处理的数据为\verb'rho'[$N_x$][$3\frac{N_y}{4}\rightarrow N_y$][$N_z$]。每个GPU只需要进行$\frac{N_y}{4} \times N_z$次变换即可。理想情况下，和单GPU运行相比，使用4个GPU运行只需要花费1/4的时间。

在X方向的傅里叶变换结束后，我们需要其他的数据来进行Y方向的运算，目前每个GPU上的数据为\verb'rho'[$N_x$][$(n-1)\frac{N_y}{4}\rightarrow (n)\frac{N_y}{4}$][$N_z$]，而进行Y方向的傅里叶变换需要的数据为\verb'rho'[$(n-1)\frac{N_x}{4}\rightarrow (n)\frac{N_x}{4}$][$N_y$][$N_z$]。我们必须对数据求转置并在GPU之间进行数据交换。而由于目前GPU与GPU之间并不能直接进行数据交换，我们不需将GPU中的数据拷贝回CPU内存中，并在CPU端进行通讯，这将需要额外的时间。所以域分解模式相比复制模式的效率高低将取决于额外的通信时间与降低的运算时间的比较。

以64*64*64个格点数，使用4个GPU为例，域分解模式求解泊松方程的具体步骤可以表示如下：
\begin{enumerate}
  \item X方向的FFT
  \begin{enumerate}
    \item 拷贝到GPU（64*16*64）
    \item FFT（64*16*64）
    \item 拷贝到CPU（64*16*64）
  \end{enumerate}
  \item 转置及CPU与CPU之间的信息通讯
  \item Y方向的FFT
  \begin{enumerate}
    \item 拷贝到GPU（64*16*64）
    \item FFT（64*16*64）
    \item 拷贝到CPU（64*16*64）
  \end{enumerate}
  \item 转置及CPU与CPU之间的信息通讯
  \item Z方向的FFT
  \begin{enumerate}
    \item 拷贝到GPU（64*16*64）
    \item FFT（64*16*64）
    \item 拷贝到CPU（64*16*64）
  \end{enumerate}
\end{enumerate}

相比较而言，复制模式的步骤为:
\begin{enumerate}
  \item X方向的FFT（64*64*64）
  \item Y方向的FFT（64*64*64）
  \item Z方向的FFT（64*64*64）
\end{enumerate}
\subsubsection{单GPU测试}
我们在单GPU上对域分解模式进行了测试，并统计了每一部分所花费的时间。测试的格点数为64*64*64，每部分所花费的时间如表\ref{tab:1GPU_Poisson}，表中的单位为秒，每部分占比如图\ref{fig:1GPU_Poisson}。

\begin{table}[!htbp]
    \centering
    \footnotesize% fontsize
    \setlength{\tabcolsep}{4pt}% column separation
    \renewcommand{\arraystretch}{1.2}%row space
    \begin{tabular}{lc}
        \hline\hline
        从CPU拷贝到GPU    & 1.306     \\
        \hline
        从GPU拷贝到CPU    & 2.979     \\
        \hline
        FFT计算           & 0.487     \\
        \hline
        转置和通讯        & 3.772     \\
        \hline
        总计              & 8.544    \\
        \hline\hline
    \end{tabular}
    \caption{单GPU域分解模式解泊松方程}
    \label{tab:1GPU_Poisson}
\end{table}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.7\textwidth]{Img/domain_decomposition_1GPU.pdf}
    \caption{单GPU域分解模式解泊松方程各部分时间占比}\label{fig:1GPU_Poisson}
\end{figure}

可以看出，FFT计算所花的时间为0.487秒，相比于额外增加的数据拷贝时间1.306 + 4.627 = 4.933秒，其占比非常小，仅仅占总时间的6\%。

多GPU的一种方式是使用同一节点内使用多个GPU，因为节点内的总线宽度限制，数据拷贝时间并不会随着使用GPU数目的增加而明显减少。而复制模式只需要花费FFT计算的时间，因为FFT计算花费的时间0.487秒远小于数据拷贝的时间4.933秒，所以即使在理想情况下使用N个GPU时FFT计算的时间减少到原来的1/N，域分解模式的总时间依然是比复制模式要大。

多GPU的另一种使用跨节点的多个GPU，即每个节点有1个GPU，通过使用多个节点来使用多个GPU，这也是目前超级计算机TITAN的运行模式。总线宽度会随着节点数增加而增加，所以数据拷贝时间会下降，假设其下降为现行的，即在N个节点上运行的话拷贝数据时间只有原来的1/N，则总时间将会是4.933/N+0.487/N，如果我们想要取得优于复制模式的速度，即4.933/N+0.487/N<0.487，GPU数目必须大于11。类似的，如果我们想要取得两倍的速度，GPU数目必须大于22。然而，这个计算忽略了CPU和CPU之间的通讯时间，一般来说，节点间的通讯时间随着节点数目增加而增加，因此我们很难取得理想情况的加速比。

下面，我们分别对节点内多GPU和跨节点GPU进行测试。

\subsubsection{单节点多GPU测试}
在同一节点中，我们进行了格点数为64*64*64的多GPU测试，表\ref{tab:2GPU_Poisson}是各个部分所花费的时间，以及和单GPU时的对比。可以看出，双GPU的总时间更大了，由8.544秒增加到了9.537秒。其中，FFT计算的时间有0.487秒减少到了0.249秒，几乎减少了一半，证明了多GPU对于减少纯FFT计算时间是确实有效的；对于数据拷贝时间，双GPU略有减少，大概减少了三分之一；总时间中增加的部分来源于转置和通讯的时间，几乎增加了一倍。
\begin{table}[!htbp]
    \centering
    \footnotesize% fontsize
    \setlength{\tabcolsep}{4pt}% column separation
    \renewcommand{\arraystretch}{1.2}%row space
    \begin{tabular}{lcc}
        \hline\hline
                          & 1GPU    & 2GPU   \\
        \hline\hline
        从CPU拷贝到GPU    & 1.306   & 0.817  \\
        \hline
        从GPU拷贝到CPU    & 2.979   & 1.991  \\
        \hline
        FFT计算           & 0.487   & 0.249  \\
        \hline
        转置和通讯        & 3.772   & 6.480  \\
        \hline
        总计              & 8.544   & 9.537  \\
        \hline\hline
    \end{tabular}
    \caption{单节点双GPU域分解模式解泊松方程}
    \label{tab:2GPU_Poisson}
\end{table}

\subsubsection{跨节点多GPU测试}
我们使用超级计算机泰坦（Titan）来进行域分解模式下的解泊松方程在跨节点多GPU上的效率。泰坦是一台由克雷公司承建的超级电脑，置放于美国能源部下属的橡树岭国家实验室中，使用由超微半导体提供的皓龙（Opteron）处理器链接英伟达提供的Tesla运算用GPU以进行协同运算。

首先，我们测试了常用的64*64*64个格点下，域分解模式下解泊松方程所花费的时间随GPU个数的变化，如图\ref{fig:TITAN_GPU_Poisson64}所示，图中蓝线为总时间，各个柱代表各个部分所花费的时间，在GPU数目很大时，图中不清楚部分可以参考表\ref{tab:TITAN_GPU_Poisson64}。

可以看出，花费的总时间随着GPU数目增加而逐渐减少，并在32个GPU的时候到达最小值，随后开始逐渐增加。其中，拷贝数据所花费的时间基本随着GPU的数目线性减少；而通讯所花费的时间先随着GPU数目增加而逐渐减少，到达极值后又开始逐渐增加，此时通讯时间占程序所花费的时间的绝大部分；FFT计算所花费的时间随着GPU数目的增加而逐渐减少，但是FFT计算占总时间的百分比很小。

\begin{figure}[!htb]
  \centering
  \begin{tabular}{|l|l|}
    \multicolumn{2}{c}{
    \includegraphics[width=0.9\textwidth]{Img/domain_decomposition_Titan64.pdf}} \\
  \end{tabular}
  \caption{64*64*64格点下，跨节点多GPU 域分解模式解泊松方程时间随GPU个数的变化}
  \label{fig:TITAN_GPU_Poisson64}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{|l|c|c|c|c|c|c|c|}
    \hline
    GPU数目	    &1	    &2	    &4	    &8 	    &16	    &32  	&64   \\
    \hline
    CPU to GPU	&2.111	&0.922	&0.390	&0.158	&0.061	&0.031	&0.0155 \\
    GPU to CPU	&1.618	&0.694	&0.314	&0.167	&0.066	&0.033	&0.0158 \\
    FFT计算  	&0.266	&0.136	&0.069	&0.039	&0.024	&0.017	&0.0150 \\
    转置和通讯	&4.02	&5.521	&2.415	&1.146	&0.691	&0.572	&0.6320 \\
    总计	    &8.015	&7.273	&3.188	&1.51	&0.842	&0.653	&0.6783 \\
    \hline
  \end{tabular}
  \caption{64*64*64格点下，跨节点多GPU 域分解模式解泊松方程各部分所用时间}
  \label{tab:TITAN_GPU_Poisson64}
\end{table}

我们也测试了更多格点数目时的情况，格点数目越多，所需要的计算量越大。当格点数为128*128*128时，程序在不同的GPU数目下消耗的时间如图\ref{fig:TITAN_GPU_Poisson128}和表\ref{tab:TITAN_GPU_Poisson128}所示，其整体的趋势和64*64*64时类似，因为更大的计算量，其总时间在使用128个GPU时到达最小值。

\begin{figure}[!htb]
  \centering
  \begin{tabular}{|l|l|}
    \multicolumn{2}{c}{
    \includegraphics[width=0.9\textwidth]{Img/domain_decomposition_Titan128.pdf}} \\
  \end{tabular}
  \caption{128*128*128格点下，跨节点多GPU域分解模式解泊松方程时间随GPU个数的变化}
  \label{fig:TITAN_GPU_Poisson128}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{|>{\small}l|c|c|c|c|c|c|c|c|c|}
    \hline
    GPU数目	    &1	    &2	    &4	    &8 	    &16	    &32  	&64     &128	&256   \\
    \hline
    CPU to GPU	&18.29	&8.687	&3.946	&2.023	&0.953	&0.404	&0.148	&0.064	&0.033 \\
    GPU to CPU	&13.11	&6.15	&2.618	&1.463	&0.795	&0.368	&0.139	&0.07	&0.036 \\
    FFT计算  	&2.185	&1.034	&0.522	&0.266	&0.138	&0.071	&0.036	&0.021	&0.017 \\
    转置和通讯	&59.99	&79.34	&51.85	&23.30	&9.797	&4.041	&2.237	&1.859	&2.506 \\
    总计	    &93.58	&95.21	&58.94	&27.05	&11.68	&4.884	&2.56	&2.014	&2.592\\
    \hline
  \end{tabular}
  \caption{128*128*128格点下，跨节点多GPU域分解模式解泊松方程各部分所用时间}
  \label{tab:TITAN_GPU_Poisson128}
\end{table}

在64*64*64和128*128*128两种情况下，总时间开始都随着GPU数目增加而减小，然后到达最小值，随后由于通讯所需要的时间变大，总时间也随之变大。
\subsubsection{小结}
通过上面对于“域分解模式”的实现和测试，可以看出其主要受限于CPU和GPU之间数据拷贝的速度与CPU和CPU之间的通讯速度。如果GPU带宽足够大，或者以后能够实现GPU与GPU之间的直接通讯的话，“域分解模式”可以作为一个可行的方案。但是在目前，与“复制模式”的比较，“域分解模式”由于其额外的数据拷贝和通讯开销，并不能通过减少运算量来达到提高速度的目的。所以我们在程序中依然使用“复制模式”，即使所有的GPU都同时运行同样的程序。

\subsection{粒子推动}
在小节\ref{section:PIC_GPU_depositor}粒子权重过程中，粒子是按照区块排列的。因此在推动粒子的时候，直接的方法是像权重过程中一样，由每个线程处理一个区块中的粒子。然而这种方法的缺陷是每个线程上的负载不均匀，可能出现某些区块中粒子数很多从而其线程运算量很大，然而另外某些区块中的粒子数较少，其线程运算量很小的情况。为了保证负载的均衡，我们采取另外一种方法，先把区块中的粒子信息重组，拷贝到一个典型的6*N的连续数组中，其中N是粒子数，6是维度；然后再并行推动。这种方法虽然需要额外的时间进行数据拷贝，但是相比第一种区块推动的负载更均匀，所以整体时间比区块推动更短，其流程如下所示：
\begin{enumerate}
  \item 将粒子从区块格式\verb"dev_ray_tile[lth*dim*npm*mth]" 重新排列，并拷贝到经典的6*N数组中\verb"dev_ray[N][6]"。
  \item 对\verb"dev_ray[N][6]"进行并行推动。
  \item 推动完成后，将粒子拷贝回原区块\verb"dev_ray_tile[lth*dim*npm*mth]" 中，以用来下一步的粒子排序和权重插值。
\end{enumerate}


\section{正确性校验}
模拟程序最重要的确保模拟结果的正确性，我们通过将GPU程序的结果与成熟的CPU程序的结果进行比较来验证GPU程序的正确性。图\ref{fig:PIC_GPU_benchmark}是CPU程序和GPU程序的发射度比较，从图中可以看出，两个程序得到的结果完全相同。通过输出数据得到，两者的区别在十的负十三次方量级，这可能是由于双精度浮点数的精度所限，双精度浮点数的精确度在十的负十四左右。而一般我们输出只取九位有效数字，所以可以认为两个程序的输出结果完全相同，GPU程序的正确性得到了验证。
\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.9\textwidth}
        \includegraphics[width=\textwidth]{Img/PIC_GPU_benchmark_x.eps}
        %\caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \includegraphics[width=\textwidth]{Img/PIC_GPU_benchmark_y.eps}
        %\caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.9\textwidth}
        \includegraphics[width=\textwidth]{Img/PIC_GPU_benchmark_z.eps}
        %\caption{}
    \end{subfigure}
    \caption{正确性校验}\label{fig:PIC_GPU_benchmark}
\end{figure}

\section{性能}

\subsection{单GPU}
首先，我们通过对比CPU程序和GPU程序的运行速度，来得到GPU程序的的加速比，加速比等于CPU程序耗时除以GPU程序耗时。测试使用的格点数为64*64*64，图\ref{fig:PIC_speedup_1GPU}和表\ref{tab:PIC_speedup_1GPU}是粒子数从16k到1.6m时CPU程序和GPU程序各部分耗时以及加速比。程序消耗的总时间的加速比大约有40，然而每一部分的加速比有很大不同。

其中，橙色柱行代表求解泊松方程，其加速比大约是64，而且并不随粒子数目的变化而变化。事实上，求解泊松方程的计算量主要和格点数相关，而此次测试中我们都是用同样的格点数，因此其耗时基本不变，之后我们会比较在不同格点数情况下求解泊松方程的加速比变化。
灰色柱行代表推动粒子的加速比，其随着粒子数目增加而变大，在粒子数较大时取得超过了70的加速比。
浅蓝色和深蓝色柱行分别是权重差值和粒子信息输出的加速比，其中GPU的权重差值部分包括了粒子排序运算，而且由于其运算的不规则性，加速比较低；而粒子信息输出部分也包含了束团参数的计算，这部分加速比较低是因为输出带宽的限制。


\begin{figure}[!htb]
  \centering
  \begin{tabular}{|l|l|}
    \multicolumn{2}{c}{
    \includegraphics[width=0.9\textwidth]{Img/PIC_speedup_1GPU.pdf}} \\
  \end{tabular}
  \caption{PIC程序在单GPU上的加速比}
  \label{fig:PIC_speedup_1GPU}
\end{figure}

\begin{table}
  \centering
  \begin{tabular}{|l|r|r|r|}
    \hline
    16k                      &    CPU(s)      &     GPU(s)    &  Speedup    \\
    \hline
    depositor (include sort) &    0.16888     &     0.05874   &  2.875043   \\
    Poisson solver           &    26.13173    &     0.42511   &  61.47051   \\
    pusher kicker            &    0.53748	  &     0.01781	  &  30.17855   \\
    output                   &    0.01977     &     0.01149   &  1.720627   \\
    total loop               &    27.78216    &     0.51199   &  54.26309   \\
    \hline
    64k                      &    CPU(s)      &     GPU(s)    &  Speedup    \\
    \hline
    depositor (include sort) &    0.3897      &     0.10847   &  3.592698   \\
    Poisson solver           &    26.08269    &     0.41554   &  62.76818   \\
    pusher kicker            &    2.00422	  &     0.04032	  &  49.70784   \\
    output                   &    0.0641      &     0.01433   &  4.473133   \\
    total loop               &    29.54123    &     0.57598   &  51.28864   \\
    \hline
    160k                     &    CPU(s)      &     GPU(s)    &  Speedup    \\
    \hline
    depositor (include sort) &    0.82705     &     0.20731   &  3.989436   \\
    Poisson solver           &    25.9208     &     0.40208   &  64.46677   \\
    pusher kicker            &    4.88644	  &     0.08343	  &  58.56934   \\
    output                   &    0.15477     &     0.02316   &  6.682642   \\
    total loop               &    32.94187    &     0.71      &  46.397     \\
    \hline
    640k                     &    CPU(s)      &     GPU(s)    &  Speedup    \\
    \hline
    depositor (include sort) &    2.79413     &     0.71529   &  3.90629    \\
    Poisson solver           &    25.72129    &     0.40045   &  64.23097   \\
    pusher kicker            &    22.48269	  &     0.31207	  &  72.04374   \\
    output                   &    0.62289     &     0.06931   &  8.987015   \\
    total loop               &    53.51193    &     1.46745   &  36.46593   \\
    \hline
    1.6m                     &    CPU(s)      &     GPU(s)    &  Speedup    \\
    \hline
    depositor (include sort) &    6.7528      &     1.73779   &  3.885855   \\
    Poisson solver           &    26.03071    &     0.39894   &  65.24969   \\
    pusher kicker            &    56.05512	  &     0.76562	  &  73.21533   \\
    output                   &    1.56528     &     0.16288   &  9.61002    \\
    total loop               &    90.40391    &     2.99053   &  30.23006   \\
    \hline
  \end{tabular}
  \caption{PIC程序在单GPU上的加速比}
  \label{tab:PIC_speedup_1GPU}
\end{table}

权重差值和粒子信息输出的的加速比较低，从而拉低了整体的加速比。整体的加速比随着粒子数的增加而逐渐变小，其原因是当粒子数目较小时的时候，求解泊松方程所占得比重较大，从而整体加速比较大；而当粒子数目变大的时候，权重差值所占的时间比重变大，因而整体加速比随之变小。程序各部分耗时在不同粒子数所占比重如图\ref{PIC_speedup_1GPU_percentage}所示。

\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=\textwidth]{Img/PIC_speedup_1GPU_percentage1.pdf}
        %\caption{}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=\textwidth]{Img/PIC_speedup_1GPU_percentage2.pdf}
        %\caption{}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.75\textwidth}
        \includegraphics[width=\textwidth]{Img/PIC_speedup_1GPU_percentage3.pdf}
        %\caption{}
    \end{subfigure}
    \caption{程序各部分耗时在不同粒子数时所占百分比}\label{fig:PIC_speedup_1GPU_percentage}
\end{figure}

在上述测试中使用的都是64*64*64个格点数，因此求解泊松方程的时间和加速比都基本不变。在不同的格点数情况下，求解泊松方程的加速比如图\ref{fig:PIC_speedup_1GPU_Poisson}所示。可以看出，求解泊松方程的加速比随着格点数的增加逐渐变大，最终达到了接近70，这是因为格点数越大，其计算量越多，GPU上的负载能够分布得更均匀。在实际模拟中常用的64*64*64个格点和128*128*128个格点中，我们都取得了令人满意的加速比。

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{Img/PIC_speedup_1GPU_Poisson.pdf}
  \caption{不同的格点数情况下单GPU求解泊松方程的加速比}
  \label{fig:PIC_speedup_1GPU_Poisson}
\end{figure}

总之，在单GPU上，程序总体的加速比达到了40，而求解泊松方程的加速比超过到了60，。在一个普通的家用机GPU上运行的速度是在64核机器上运行的两倍。
另外，如小节\ref{section:PIC_GPU_reorder}所述，因为GPU内存大小一般是固定的，而不像CPU内存那样较容易的扩展，所以单GPU的粒子数目存在某个上限，当超过最大粒子数时，应该使用多GPU来进行计算。在接下来的一节中，我们将讨论PIC程序在多GPU上的效率。

\subsection{多GPU}
我们使用超级计算机Titan进行多GPU测试。Titan中每个节点只有一个GPU，型号为NVIDIA K20x，因此只能通过使用多个节点来使用多个GPU。在这种条件下，GPU之间的通讯只能够通过先将数据拷贝到CPU端，再通过CPU端的节点间网络进行通讯。

图\ref{fig:PIC_speedup_Titan_160k_1}为复制模式求解泊松方程时各个部分所花费的时间随GPU个数的变化，可以看出，因为处于复制模式，求解泊松方程的耗时基本保持不变；由于跟多的GPU个数意味着每个GPU上的粒子数随之减小，粒子推动的耗时随着GPU数目的增加而减少；而由于通讯成本随着GPU数目增加而增加，信息统计和输出耗时也随之增加。各个部分对于GPU的数目并不相同，总体来看，总耗时基本保持不变。

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{Img/PIC_speedup_Titan_160k_1.pdf}
  \caption{64*64*64个格点，160k个粒子时，复制模式程序耗时随GPU个数的变化}
  \label{fig:PIC_speedup_Titan_160k_1}
\end{figure}

图\ref{fig:PIC_speedup_Titan_160k_2}和图\ref{fig:PIC_speedup_Titan_160k_1}类似，也是64*64*64各个点，160k个粒子下程序总耗时随着GPU数目的变化。但是在图\ref{fig:PIC_speedup_Titan_160k_2}中，GPU数目大于等于2的时候，求解泊松方程使用域分解模式。其在各种情况下，都相较复制模式耗时更多，主要是拷贝数据花费了大量时间，这个结果和我们在小节\ref{section:PIC_GPU_Poisson}中讨论的相符。因此在之后的测试中，我们都使用复制模式。

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{Img/PIC_speedup_Titan_160k_2.pdf}
  \caption{64*64*64个格点，160k个粒子时，域分解模式程序耗时随GPU个数的变化}
  \label{fig:PIC_speedup_Titan_160k_2}
\end{figure}

当粒子数更大时，耗时的趋势发生了变化。图\ref{fig:PIC_speedup_Titan_1_6m}是粒子数为一百六十万（1.6m）的结果，粒子数较图\ref{fig:PIC_speedup_Titan_160k_1}变大了十倍。在使用1.6m个粒子时，总耗时随着使用的GPU数目增加而明显下降，并在32个GPU处到达最小值。
这是因为在大粒子数目情况下，推动粒子和权重差值所占用的时间占了总时间的绝大部分。而由于使用多GPU带来的每个GPU上的粒子数目变少，推动粒子和权重差值能够很好的被多GPU加速。

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{Img/PIC_speedup_Titan_1_6m.pdf}
  \caption{64*64*64个格点，1.6m个粒子时，程序耗时随GPU个数的变化}
  \label{fig:PIC_speedup_Titan_1_6m}
\end{figure}

我们尝试使用更大的粒子数，一千六百万个粒子（16m），更十倍于前，如图\ref{fig:PIC_speedup_Titan_16m}所示。由于GPU内存大小的限制，程序在这个粒子数下不能仅仅使用1或2个GPU运行，因此在图\ref{fig:PIC_speedup_Titan_16m}中，GPU数目等于1和等于2的时候没有数据。

超级计算机Titan上使用的GPU型号为NVIDIA K20x，每个GPU有5GB的内存。理想情况下，一个5GB内存能够使用的最大粒子数为六千万（60m）。但这个粒子数是在空间均匀分布下才能使用，而实际上，我们很难达到这个粒子数。一个实际的加速器模拟中我们通常使用KV，WaterBag，或者高斯分布，因此其可用的粒子数目远小于理想数目。

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{Img/PIC_speedup_Titan_16m.pdf}
  \caption{64*64*64个格点，16m个粒子时，程序耗时随GPU个数的变化}
  \label{fig:PIC_speedup_Titan_16m}
\end{figure}

\section{CPU集群}
为了和GPU做比较，我们也在CPU集群上实现了PIC程序，其使用的计算机为Cori Knight Landing，使用的是Intel最新的众核处理器，每个处理器有68个核心。
我们的程序使用和MPI和OpenMP混合并行，以提高运行的效率。

首先，我们在一个节点上研究不同OpenMP线程数和MPI进程数下程序的效率和内存占用情况，并找到最优混合并行配置。之后，我们使用多个节点，研究程序在不同节点数下的效率变化。

\subsection{单节点}
首先，我们使用64*64*64个网格，在1.6m粒子数下测试不同的混合并行配置，如图\ref{fig:PIC_speedup_Cori_1node_1_6m}所示。其中横轴为不同的并行配置，MPI进程数由1指数增加到64，而OpenMP线程数有64指数得减小到1，线程数乘以进程数则保持不变。在每种并行配置下，总时间（浅蓝色实线），和权重差值，求解泊松方程，推动粒子，信息输出（各色柱行）所耗时间由左纵轴以秒为单位表示，而内存占用（绿色实线）由右纵轴以GB为单位表示。

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{Img/PIC_speedup_Cori_1node_1_6m.pdf}
  \caption{1.6m粒子数下，不同混合并行配置的耗时与内存占用}
  \label{fig:PIC_speedup_Cori_1node_1_6m}
\end{figure}

从图\ref{fig:PIC_speedup_Cori_1node_1_6m}可以看出，除了纯OpenMP并行（MPI=1，OMP=64）明显较慢外，其他各个并行配置下所消耗的总时间的差别并不大，
其中耗时最小的并行配置为使用32个MPI进程，每个进程使用2个OpenMP线程。一般而言，使用较大的MPI进程数是比较有效率的选择。
而内存占用基本随着MPI进程数目线性增加。



程序的不同部分对于并行配置的反应并不相同。随着MPI进程数的增加和每个进程所用线程数的减小，权重差值的耗时先减小后增加，一开始先从MPI=64，OMP=1时的1.14秒减少到了MPI = 8，OMP =8时的0.58秒，然后其开始剧烈增加，最终到达MPI = 1，OMP =64时的4.85秒。其原因是权重差值需要在不同的线程之间进行归约以避免线程冲突，而归约操作在线程数很大时有一个较大的启动时间。粒子推动耗时随着MPI数变大单调增加，这是因为OpenMP更适合Knight Landing众核架构，并能更有效的利用矢量处理器。

我们也测试了不同的粒子数下的并行配置，测试的粒子数为160k和16m，分别是之前粒子数的十分之一和十倍，其结果如图\ref{fig:PIC_speedup_Cori_1node_160k}和图\ref{fig:PIC_speedup_Cori_1node_16m}所示。
在160k个粒子的情况下，总时间随着MPI进程数的增加单调减少，这表明小粒子数情况更适合纯MPI并行。
而在1.6m个粒子的情况下，纯OpenMP并行由于无MPI进程间通讯，显示出一些优势，但是其总耗时依然大于MPI并行，其耗时最小的配置为MPI=64，OMP=1。

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{Img/PIC_speedup_Cori_1node_160k.pdf}
  \caption{160k粒子数下，不同混合并行配置的耗时与内存占用}
  \label{fig:PIC_speedup_Cori_1node_160k}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{Img/PIC_speedup_Cori_1node_16m.pdf}
  \caption{16m粒子数下，不同混合并行配置的耗时与内存占用}
  \label{fig:PIC_speedup_Cori_1node_16m}
\end{figure}

在各种情况下，无论粒子数多少，内存占用总是随着MPI进程数变大而单调增加。
\subsection{多节点}
从上面单节点的测试中可以得到，使用较大的MPI进程数和较小的OpenMP线程数是更有效率的并行配置。
因此在多节点的测试中，我们首先选用纯MPI并行，测试在不同的节点数下程序总体以及各个部分的耗时情况。之后，我们也测试了OMP=2和OMP=4的情况，并与纯MPI程序进行了比较。

图\ref{fig:PIC_speedup_Cori_scalability}是纯MPI配置下PIC程序在CPU集群多节点下的耗时情况。每个节点我们使用64个核，图\ref{fig:PIC_speedup_Cori_scalability}的横轴为节点数目；左纵轴为时间，以秒为单位；右纵轴为内存使用，以GB为单位。随着使用更多的节点，程序总耗时先降低后增加，在32个节点处到达最小值。总耗时先减小是因为使用的节点数越多，每个节点上需要进行的运算越少；后增加是因为随着节点数上升，节点间的通讯时间也会随之增加。图\ref{fig:PIC_speedup_Cori_percetage_64nodes}为使用64个节点时程序各个部分消耗时间所占的百分比，可以看出此时通讯耗时已经占了总耗时的60\%。

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{Img/PIC_speedup_Cori_scalability.pdf}
  \caption{PIC程序使用多个CPU节点的耗时}
  \label{fig:PIC_speedup_Cori_scalability}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.5\textwidth]{Img/PIC_speedup_Cori_percetage_64nodes.pdf}
  \caption{使用64个节点时程序各个部分消耗时间所占的百分比}
  \label{fig:PIC_speedup_Cori_percetage_64nodes}
\end{figure}

接下来，我们对OMP=2和OMP=4的并行配置进行了测试，并与纯MPI程序（OMP=1）进行了比较，如图\ref{fig:PIC_speedup_Cori_multi_nodes_timeMemory}所示。
在大部分情况下，不同并行的耗时差别很小；但是在某些情况下差别很大，比如节点数为16时，纯MPI的速度是OMP=4的1.8倍。
对于内存使用情况， 使用更多的OpenMP线程和更少的MPI进程总是占优势。
综合考虑，在内存足够大的情况下，在Knight Landing上运行的PIC程序使用纯MPI并行配置依然是一个很好的选择。

\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{Img/PIC_speedup_Cori_multi_nodes_time.pdf}
        \caption{不同并行配置下的多节点运行耗时比较}
    \end{subfigure}
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{Img/PIC_speedup_Cori_multi_nodes_memory.pdf}
        \caption{不同并行配置下的多节点运行内存比较}
    \end{subfigure}
    \caption{不同并行配置下的多节点运行情况比较}\label{fig:PIC_speedup_Cori_multi_nodes_timeMemory}
\end{figure}

\subsection{与GPU对比}

在本章之前的测试中，我们使用的GPU型号为Nvidia GeForce GTX 1060，其包含1280颗核心，时钟频率为1.71GHz。而我们使用的CPU型号为Intel® Xeon Phi™ Processor 7250 ("Knights Landing")，共包含68颗核心，时钟频率为1.40GHz，但是实际上，我们只是使用了64颗核心。

而由于GPU程序做了一些简化，缺少一部分步骤，例如粒子丢失判据等，我们在进行比较的时候也在CPU程序的耗时中减去了相应部分。
在64*64*64个格点数，1.6m个粒子的情况下，对于同样长度的加速器，使用一个GPU代码的总耗时为3.56秒，这类似于CPU程序在300个核心上的运行时间。换而言之，对于我们的PIC程序，一个GPU卡的运算效率与4到8个CPU节点的运行效率相当。

\section{小节}

我们在GPU上使用使用CUDA库实现了基于PIC方法的多粒子模拟程序，并对如何避免竞争条件和实现更高性能的GPU代码结构和并行策略进行了介绍。在单个GPU卡上，我们使用普通的家用GPU（GTX 1060）实现了超过50倍的加速。当粒子数较大时，程序在GPU集群上也显示出良好的可扩展性；而当粒子数目较小时其可扩展性较差。
我们也新的CPU架构Cori Knight Landing上实现了PIC程序，并探索了其最佳性能的并行线程配置。通过比较，单GPU运行的程序和使用4或8个节点的程序性能相当。
在未来的研究中，我们将继续开发此代码并提高效率。

%\begin{appendices}
%\lstinputlisting[language=c++, caption=Depositor using cuda]{code/Depositor_cuda.cu}
%\end{appendices} 